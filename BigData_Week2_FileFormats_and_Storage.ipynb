{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà (Big Data) ‚Äî Week 2: File Formats & Modern Storage\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/witsarutsarai12-Academic/128-356-Big-Data/blob/main/BigData_Week2_FileFormats_and_Storage.ipynb)\n",
    "\n",
    "**Learning outcomes:**\n",
    "- ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á File System vs Database ‡πÅ‡∏•‡∏∞ Data Lake vs Lakehouse\n",
    "- ‡πÅ‡∏¢‡∏Å‡πÅ‡∏¢‡∏∞ Row-format vs Columnar-format ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û (compression, column pruning, predicate pushdown)\n",
    "- ‡πÅ‡∏õ‡∏•‡∏á CSV ‚Üí Parquet ‡πÅ‡∏•‡∏∞ benchmark ‡∏Ç‡∏ô‡∏≤‡∏î/‡πÄ‡∏ß‡∏•‡∏≤‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÑ‡∏î‡πâ\n",
    "- ‡πÉ‡∏ä‡πâ DuckDB/Pandas ‡∏≠‡πà‡∏≤‡∏ô Parquet ‡πÅ‡∏•‡∏∞‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö mini-lab ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏ò‡∏¥‡∏ï‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ea4002",
   "metadata": {},
   "source": [
    "## Part 0: ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°\n",
    "- ‡∏£‡∏±‡∏ô‡∏ö‡∏ô Google Colab ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ RAM ‚â• 8GB\n",
    "- ‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ: `pandas`, `numpy`, `pyarrow`/`fastparquet`, `duckdb`\n",
    "- ‡∏ñ‡πâ‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏á‡πÄ‡∏õ‡∏¥‡∏î Colab ‡πÉ‡∏´‡∏°‡πà ‡πÉ‡∏´‡πâ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡∏Å‡πà‡∏≠‡∏ô\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958f9727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô Python\n",
    "import sys\n",
    "print(sys.version)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baad2746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ (‡∏£‡∏±‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏û‡∏≠)\n",
    "%pip -q install pandas pyarrow fastparquet duckdb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a33d31c",
   "metadata": {},
   "source": [
    "## Part 1: The Computer & Data ‚Äî Mental Model\n",
    "- **Storage (SSD/HDD)** = ‡∏ï‡∏π‡πâ‡πÄ‡∏¢‡πá‡∏ô/‡∏´‡πâ‡∏≠‡∏á‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡∏≠‡∏á ‚Äî ‡∏à‡∏∏‡πÄ‡∏¢‡∏≠‡∏∞, ‡∏ä‡πâ‡∏≤, ‡∏õ‡∏¥‡∏î‡πÑ‡∏ü‡∏Ç‡∏≠‡∏á‡∏¢‡∏±‡∏á‡∏≠‡∏¢‡∏π‡πà\n",
    "- **RAM** = ‡πÇ‡∏ï‡πä‡∏∞‡πÄ‡∏Ç‡∏µ‡∏¢‡∏á ‚Äî ‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡πâ‡∏≠‡∏¢‡πÅ‡∏ï‡πà‡πÄ‡∏£‡πá‡∏ß ‡∏ï‡πâ‡∏≠‡∏á‡∏¢‡∏Å‡∏Ç‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏ï‡∏π‡πâ‡πÄ‡∏¢‡πá‡∏ô‡∏°‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏ñ‡∏∂‡∏á‡∏à‡∏∞‡∏õ‡∏£‡∏∏‡∏á‡πÑ‡∏î‡πâ\n",
    "- **CPU** = ‡∏û‡πà‡∏≠‡∏Ñ‡∏£‡∏±‡∏ß ‚Äî ‡∏•‡∏á‡∏°‡∏∑‡∏≠‡∏´‡∏±‡πà‡∏ô/‡∏ú‡∏±‡∏î\n",
    "- **‡∏Å‡∏é‡πÄ‡∏´‡∏•‡πá‡∏Å**: ‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏π‡∏Å‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å Storage ‚Üí RAM ‚Üí CPU ‡πÄ‡∏™‡∏°‡∏≠\n",
    "\n",
    "![Kitchen Analogy](images/kitchen_analogy.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bdd3d3",
   "metadata": {},
   "source": [
    "### Key Terms ‚Äî Storage & Filesystems\n",
    "- **Block/Cluster**: ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡πÄ‡∏•‡πá‡∏Å‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà disk ‡πÉ‡∏ä‡πâ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡πÄ‡∏ä‡πà‡∏ô 4KB)\n",
    "- **Inode/Metadata**: ‡∏™‡∏°‡∏∏‡∏î‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå (‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå, ‡πÄ‡∏ß‡∏•‡∏≤, ‡∏Ç‡∏ô‡∏≤‡∏î, ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á block)\n",
    "- **Page Cache**: RAM ‡∏ó‡∏µ‡πà OS ‡πÅ‡∏≠‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏á‡∏≠‡πà‡∏≤‡∏ô ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡πà‡∏≤‡∏ô‡∏ã‡πâ‡∏≥‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô\n",
    "- **Sequential vs Random I/O**: ‡∏≠‡πà‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤‡∏≠‡πà‡∏≤‡∏ô‡∏Å‡∏£‡∏∞‡πÇ‡∏î‡∏î‡πÄ‡∏¢‡∏≠‡∏∞ ‡πÜ\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a82c019",
   "metadata": {},
   "source": [
    "### File System Internals\n",
    "- ‡πÑ‡∏ü‡∏•‡πå‡∏ñ‡∏π‡∏Å‡∏´‡∏±‡πà‡∏ô‡πÄ‡∏õ‡πá‡∏ô **Block/Cluster** (‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà 4KB)\n",
    "- ‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏•‡πá‡∏Å‡∏°‡∏≤‡∏Å‡∏Å‡∏¥‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏ö‡∏•‡πá‡∏≠‡∏Å ‚Üí **Slack space**\n",
    "- ‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà‡∏ñ‡∏π‡∏Å‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏´‡∏•‡∏≤‡∏¢‡∏ö‡∏•‡πá‡∏≠‡∏Å ‚Üí **Fragmentation**\n",
    "- ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏•‡πá‡∏Å‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å (small files problem) ‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡πâ‡∏≠‡∏á‡∏ß‡∏¥‡πà‡∏á‡∏´‡∏≤ block ‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á ‚Üí ‡∏ä‡πâ‡∏≤\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b783d92e",
   "metadata": {},
   "source": [
    "### File Read Path (‡πÄ‡∏õ‡∏¥‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Å‡∏¥‡∏î‡∏≠‡∏∞‡πÑ‡∏£‡∏Ç‡∏∂‡πâ‡∏ô‡∏ö‡πâ‡∏≤‡∏á?)\n",
    "1) `open()` ‚Üí OS ‡∏´‡∏≤ metadata (inode)\n",
    "2) ‡πÅ‡∏õ‡∏•‡∏á filename ‚Üí block list\n",
    "3) OS ‡∏î‡∏∂‡∏á block ‡∏à‡∏≤‡∏Å disk ‚Üí RAM (page cache)\n",
    "4) ‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏≠‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å RAM (‡πÑ‡∏°‡πà‡∏≠‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å disk ‡∏ï‡∏£‡∏á ‡πÜ)\n",
    "\n",
    "![File Read Path](images/file_read_path.png)\n",
    "\n",
    "**Key insight:** ‡∏ñ‡πâ‡∏≤‡∏≠‡πà‡∏≤‡∏ô‡∏ã‡πâ‡∏≥ ‡πÅ‡∏•‡∏∞‡∏¢‡∏±‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô page cache ‚Üí ‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23c906a",
   "metadata": {},
   "source": [
    "### Fragmentation & Sequential Read\n",
    "- ‡∏ñ‡πâ‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á ‚Üí disk ‡∏ï‡πâ‡∏≠‡∏á ‚Äú‡∏Å‡∏£‡∏∞‡πÇ‡∏î‡∏î‡∏´‡∏±‡∏ß‡∏≠‡πà‡∏≤‡∏ô‚Äù ‚Üí ‡∏ä‡πâ‡∏≤‡∏•‡∏á\n",
    "- ‡∏£‡∏∞‡∏ö‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏°‡∏±‡∏¢‡πÉ‡∏´‡∏°‡πà‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏à‡∏±‡∏î‡πÉ‡∏´‡πâ‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏Å‡∏±‡∏ô ‡πÅ‡∏ï‡πà‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏•‡πá‡∏Å‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å‡∏ó‡∏≥‡πÉ‡∏´‡πâ fragmentation ‡πÄ‡∏û‡∏¥‡πà‡∏°\n",
    "\n",
    "![Disk Fragmentation](images/disk_fragmentation.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b9337a",
   "metadata": {},
   "source": [
    "### Distributed File Systems (HDFS / S3) ‚Äî ‡∏†‡∏≤‡∏û‡πÉ‡∏´‡∏ç‡πà‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô Block & Metadata\n",
    "- **HDFS**: ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô block ~128MB ‡∏û‡∏£‡πâ‡∏≠‡∏° **replication factor = 3** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏±‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏•‡πà‡∏°; NameNode ‡πÄ‡∏Å‡πá‡∏ö metadata ‡πÑ‡∏ß‡πâ‡πÉ‡∏ô RAM ‡∏à‡∏∂‡∏á‡πÄ‡∏Å‡∏•‡∏µ‡∏¢‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏¥‡πã‡∏ß‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å\n",
    "- **‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÑ‡∏ü‡∏•‡πå**: client buffer ‡πÑ‡∏ß‡πâ‡∏à‡∏ô‡πÄ‡∏ï‡πá‡∏° block ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏≠‡∏¢‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠ DataNode ‚Üí ‡∏•‡∏î network round-trip\n",
    "- **Object Storage (S3/MinIO)**: ‡πÑ‡∏°‡πà‡∏°‡∏µ concept block ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡πÄ‡∏à‡∏≠‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏•‡πá‡∏Å ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡∏¥‡∏î request ‡∏ó‡∏µ‡∏•‡∏∞‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡∏∞ metadata lookup ‡∏´‡∏ô‡∏±‡∏Å\n",
    "- **‡∏Ç‡πâ‡∏≠‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏ä‡∏¥‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥**: ‡πÄ‡∏•‡πá‡∏á‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå Parquet 128‚Äì512MB ‡∏ï‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå ‡πÅ‡∏•‡∏∞‡∏à‡∏≥‡∏ô‡∏ß‡∏ô partition ‡πÉ‡∏´‡πâ‡∏û‡∏≠‡∏î‡∏µ‡∏Å‡∏±‡∏ö‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç query (‡πÄ‡∏ä‡πà‡∏ô ‡∏ß‡∏±‡∏ô/‡πÄ‡∏î‡∏∑‡∏≠‡∏ô ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà user_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b7c311",
   "metadata": {},
   "source": [
    "### HDFS vs Object Storage (S3) ‚Äî ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡∏¢‡∏±‡∏á‡πÑ‡∏á\n",
    "| ‡∏°‡∏¥‡∏ï‡∏¥ | HDFS | S3/Object Storage |\n",
    "|---|---|---|\n",
    "| ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• | block | object | \n",
    "| Metadata | NameNode (RAM-heavy) | Service metadata | \n",
    "| Latency | ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡πÉ‡∏ô cluster | ‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢ | \n",
    "| Update/Append | ‡∏ó‡∏≥‡πÑ‡∏î‡πâ (append ‡∏à‡∏≥‡∏Å‡∏±‡∏î) | ‡∏õ‡∏Å‡∏ï‡∏¥ replace ‡∏ó‡∏±‡πâ‡∏á object | \n",
    "\n",
    "![HDFS vs S3](images/hdfs_vs_s3.png)\n",
    "\n",
    "**‡∏™‡∏£‡∏∏‡∏õ:** HDFS ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö on-prem cluster; S3 ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö cloud scale + durability\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dd21d0",
   "metadata": {},
   "source": [
    "#### Small Files Problem: ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£ & ‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ (Deep Dive)\n",
    "\n",
    "**‡∏≠‡∏≤‡∏Å‡∏≤‡∏£ (Symptoms)**:\n",
    "- **NameNode / Metadata Service Overload**: ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡πÄ‡∏£‡∏≤‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå 1TB ‡∏ñ‡πâ‡∏≤‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏õ‡πá‡∏ô 1 ‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà = 1 metadata entry. ‡πÅ‡∏ï‡πà‡∏ñ‡πâ‡∏≤‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏õ‡πá‡∏ô 1 ‡∏•‡πâ‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏•‡πá‡∏Å (1MB/file) = 1 ‡∏•‡πâ‡∏≤‡∏ô entries ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏à‡∏≥‡πÉ‡∏ô RAM ‡∏Ç‡∏≠‡∏á NameNode -> ‡∏£‡∏∞‡∏ö‡∏ö‡∏•‡πà‡∏° (OOM).\n",
    "- **Slow List/Read**: ‡∏Å‡∏≤‡∏£ list file 1 ‡∏•‡πâ‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏Å‡∏¥‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏≤‡∏ô‡∏°‡∏≤‡∏Å ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏™‡∏µ‡∏¢‡πÄ‡∏ß‡∏•‡∏≤ \"Open/Close\" ‡πÑ‡∏ü‡∏•‡πå‡∏ö‡πà‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏ß‡∏•‡∏≤‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏£‡∏¥‡∏á\n",
    "\n",
    "**Analogy (‡∏≠‡∏∏‡∏õ‡∏°‡∏≤)**:\n",
    "- ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÑ‡∏õ‡∏ã‡∏∑‡πâ‡∏≠‡∏Ç‡∏≠‡∏á 1,000 ‡∏ä‡∏¥‡πâ‡∏ô: \n",
    "   - **Large File**: ‡πÑ‡∏î‡πâ‡πÉ‡∏ö‡πÄ‡∏™‡∏£‡πá‡∏à‡∏¢‡∏≤‡∏ß 1 ‡πÉ‡∏ö (‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏á‡πà‡∏≤‡∏¢)\n",
    "   - **Small Files**: ‡πÑ‡∏î‡πâ‡πÉ‡∏ö‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏±‡πâ‡∏ô‡πÜ 1,000 ‡πÉ‡∏ö (‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏¢‡∏≤‡∏Å, ‡∏´‡∏≤‡∏¢‡∏á‡πà‡∏≤‡∏¢, ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ä‡πâ‡∏≤)\n",
    "\n",
    "![Small Files Bottleneck](images/small_files_bottleneck.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830ad08e",
   "metadata": {},
   "source": [
    "### Real-World Use Case: Log Data Platform (Shopee/Lazada style)\n",
    "**‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå**: ‡∏°‡∏µ User click logs ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ‡∏•‡∏∞ 100,000 events\n",
    "- **‡∏ñ‡πâ‡∏≤ Save ‡∏ó‡∏∏‡∏Å‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ**: ‡∏à‡∏∞‡πÑ‡∏î‡πâ‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏•‡πá‡∏Å ‡πÜ ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏´‡∏≤‡∏®‡∏≤‡∏• (Small Files Problem) ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡πÑ‡∏°‡πà‡∏Å‡∏µ‡πà‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á\n",
    "- **‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ**: ‡πÉ‡∏ä‡πâ Buffer (Kafka/Memory) ‡∏£‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö 5-10 ‡∏ô‡∏≤‡∏ó‡∏µ ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏£‡∏ö 500MB ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏≠‡∏¢‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏•‡∏á Storage ‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà 1 ‡πÑ‡∏ü‡∏•‡πå (Batch Write)\n",
    "\n",
    "### Scenario Exercise ‚Äî ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏à‡∏±‡∏î‡πÑ‡∏ü‡∏•‡πå\n",
    "1) ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• log ‡∏£‡∏≤‡∏¢‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á 1MB ‡∏ï‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå ‡∏ï‡∏•‡∏≠‡∏î‡∏õ‡∏µ ‚Üí ‡∏à‡∏∞ partition ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?  \n",
    "2) ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ò‡∏∏‡∏£‡∏Å‡∏£‡∏£‡∏° 200GB/‡∏ß‡∏±‡∏ô ‡∏ï‡πâ‡∏≠‡∏á query ‡∏£‡∏≤‡∏¢‡∏ß‡∏±‡∏ô ‚Üí ‡∏ï‡∏±‡πâ‡∏á‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏£?  \n",
    "3) ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏Ñ‡πà‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå `status` ‡∏Å‡∏±‡∏ö `date` ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å ‚Üí ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏Å‡πá‡∏ö‡πÅ‡∏ö‡∏ö‡πÑ‡∏´‡∏ô?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f2de53",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>üí° ‡πÄ‡∏â‡∏•‡∏¢‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ (Scenario Exercise) - ‡∏Ñ‡∏•‡∏¥‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö</b></summary>\n",
    "\n",
    "1. **Log ‡∏£‡∏≤‡∏¢‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á 1MB/file**: \n",
    "   - ‚ùå **‡∏´‡πâ‡∏≤‡∏° Partition ‡∏£‡∏≤‡∏¢‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á**: ‡∏à‡∏∞‡πÄ‡∏Å‡∏¥‡∏î‡πÑ‡∏ü‡∏•‡πå 1MB ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏õ‡∏µ‡∏•‡∏∞ 8,760 ‡πÑ‡∏ü‡∏•‡πå (Small Files!)\n",
    "   - ‚úÖ **‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ**: ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏•‡∏á temp folder ‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏ä‡πâ process ‡∏ó‡∏≥ **Compaction** ‡∏£‡∏ß‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏≤‡∏¢‡∏ß‡∏±‡∏ô (Day Partition) ‡∏´‡∏£‡∏∑‡∏≠‡∏£‡∏≤‡∏¢‡πÄ‡∏î‡∏∑‡∏≠‡∏ô ‡πÉ‡∏´‡πâ‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà‡∏Ç‡∏∂‡πâ‡∏ô (‡πÄ‡∏ä‡πà‡∏ô ~100MB+)\n",
    "\n",
    "2. **Transaction 200GB/‡∏ß‡∏±‡∏ô**: \n",
    "   - ‚úÖ **‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢**: ‡∏Ñ‡∏ß‡∏£‡∏ï‡∏±‡∏î‡∏ó‡∏µ‡πà **128MB - 512MB** ‡∏ï‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå\n",
    "   - ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Engine ‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏ö‡∏ö Parallel ‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (‡∏ñ‡πâ‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà 200GB ‡∏Å‡πâ‡∏≠‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏≠‡πà‡∏≤‡∏ô‡∏ï‡∏≤‡∏¢ ‡πÅ‡∏ï‡πà‡∏ñ‡πâ‡∏≤‡∏ã‡∏≠‡∏¢‡πÄ‡∏õ‡πá‡∏ô 100 ‡πÑ‡∏ü‡∏•‡πå 2GB ‡∏Å‡πá‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡πÉ‡∏´‡πâ 100 cores ‡∏ä‡πà‡∏ß‡∏¢‡∏Å‡∏±‡∏ô‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏î‡πâ)\n",
    "\n",
    "3. **Query ‡πÅ‡∏Ñ‡πà Status/Date**: \n",
    "   - ‚úÖ **‡πÉ‡∏ä‡πâ Parquet**: ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏õ‡πá‡∏ô Columnar Format\n",
    "   - ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏Ñ‡πà 2 ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏Ç‡∏≠ ‡πÅ‡∏•‡πâ‡∏ß‡∏Ç‡πâ‡∏≤‡∏°‡∏≠‡∏µ‡∏Å 98 ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠ (Column Pruning) ‚Üí ‡πÄ‡∏£‡πá‡∏ß‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡πÄ‡∏á‡∏¥‡∏ô‡∏Ñ‡πà‡∏≤ Read API\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7a21f7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48282000",
   "metadata": {},
   "source": [
    "### Activity 1 ‚Äî Small Files Problem (‡∏•‡∏á‡∏°‡∏∑‡∏≠‡πÄ‡∏≠‡∏á)\n",
    "‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏•‡πá‡∏Å‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å ‡πÅ‡∏•‡πâ‡∏ß‡∏î‡∏π‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏ö‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ zip ‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡πà‡∏≤‡∏ô‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå\n",
    "- ‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á: ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏•‡πå 5,000 ‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏ô‡∏≤‡∏î 1KB ‡∏°‡∏±‡∏Å‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Ç‡∏ô‡∏≤‡∏î 5MB\n",
    "- ‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡πâ‡∏ß‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï‡πÄ‡∏ß‡∏•‡∏≤\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b730f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏¥‡πã‡∏ß‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å vs ‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡πÅ‡∏•‡πâ‡∏ß‡∏ß‡∏±‡∏î‡πÄ‡∏ß‡∏•‡∏≤ listdir / zip\n",
    "import os, tempfile, time, shutil, pathlib\n",
    "\n",
    "N_FILES = 2000   # ‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏•‡πá‡∏Å‡πÜ\n",
    "SMALL_BYTES = 1024  # 1KB ‡∏ï‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå\n",
    "\n",
    "base = pathlib.Path(tempfile.mkdtemp())\n",
    "small_dir = base / \"small\"\n",
    "big_dir = base / \"big\"\n",
    "small_dir.mkdir(); big_dir.mkdir()\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏•‡πá‡∏Å‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å\n",
    "start = time.time()\n",
    "for i in range(N_FILES):\n",
    "    (small_dir / f\"f_{i}.txt\").write_bytes(b\"a\" * SMALL_BYTES)\n",
    "small_elapsed = time.time() - start\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà‡∏Å‡πâ‡∏≠‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô\n",
    "big_path = big_dir / \"one_big.txt\"\n",
    "start = time.time()\n",
    "big_path.write_bytes(b\"a\" * SMALL_BYTES * N_FILES)\n",
    "big_elapsed = time.time() - start\n",
    "\n",
    "print(f\"‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏•‡πá‡∏Å {N_FILES} ‡πÑ‡∏ü‡∏•‡πå ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤ {small_elapsed:.2f}s\")\n",
    "print(f\"‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤ {big_elapsed:.2f}s\")\n",
    "\n",
    "# ‡∏ß‡∏±‡∏î‡πÄ‡∏ß‡∏•‡∏≤ listdir (metadata lookup)\n",
    "start = time.time(); len(list(small_dir.iterdir())); ls_small = time.time() - start\n",
    "start = time.time(); len(list(big_dir.iterdir())); ls_big = time.time() - start\n",
    "print(f\"listdir ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏•‡πá‡∏Å ‡πÉ‡∏ä‡πâ {ls_small*1000:.1f} ms; ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà ‡πÉ‡∏ä‡πâ {ls_big*1000:.1f} ms\")\n",
    "\n",
    "shutil.rmtree(base)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df448491",
   "metadata": {},
   "source": [
    "### File System vs Database (‡∏ó‡∏≥‡πÑ‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ DB?)\n",
    "| ‡∏°‡∏¥‡∏ï‡∏¥ | File (CSV/TXT) | Database (MySQL/Postgres) |\n",
    "|---|---|---|\n",
    "| ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠ | ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÑ‡∏°‡πà‡∏à‡∏ö‡πÑ‡∏ü‡∏î‡∏±‡∏ö = ‡πÑ‡∏ü‡∏•‡πå‡∏û‡∏±‡∏á | **ACID** ‡∏Ñ‡∏∏‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á | \n",
    "| ‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ | ‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡πà‡∏≤‡∏ô‡∏ó‡∏∏‡∏Å‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î (scan) | ‡∏°‡∏µ **Index** ‡∏Å‡∏£‡∏∞‡πÇ‡∏î‡∏î‡πÑ‡∏õ‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ |\n",
    "| ‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç | ‡πÅ‡∏Å‡πâ 1 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î = ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏±‡πâ‡∏á‡πÑ‡∏ü‡∏•‡πå | ‡πÅ‡∏Å‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ row ‡πÑ‡∏î‡πâ |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65fc611",
   "metadata": {},
   "source": [
    "### CSV Reality Check ‚Äî ‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏±‡∏Å‡∏û‡∏±‡∏á\n",
    "- **Delimiter/encoding ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô**: `,` vs `;` vs `\t`, UTF-8 vs Windows-874 ‚Üí ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏û‡∏µ‡πâ‡∏¢‡∏ô\n",
    "- **Quoted fields**: ‡∏Ñ‡πà‡∏≤ string ‡∏ó‡∏µ‡πà‡∏°‡∏µ comma / newline ‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏π‡∏Å‡∏•‡πâ‡∏≠‡∏°‡∏î‡πâ‡∏ß‡∏¢ `\"\"`; ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏™‡πà ‚Üí column shift\n",
    "- **Missing header / column drift**: ‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ô‡∏•‡∏∞‡∏£‡∏∞‡∏ö‡∏ö‡∏≠‡∏≤‡∏à‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô\n",
    "- **Type drift**: ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡∏°‡∏µ leading zero (`00123`) ‡∏ñ‡∏π‡∏Å‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô int ‚Üí ‡∏®‡∏π‡∏ô‡∏¢‡πå‡∏´‡∏≤‡∏¢\n",
    "- ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á: ‡∏£‡∏∞‡∏ö‡∏∏ `dtype` ‡∏ä‡∏±‡∏î, ‡πÉ‡∏ä‡πâ `quotechar`/`escapechar`, ‡∏ï‡∏£‡∏ß‡∏à schema ‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ, ‡∏ñ‡πâ‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà ‚Üí ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô Parquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7835009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏±‡∏ç‡∏´‡∏≤ CSV: comma ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "raw = \"id,name,comment\n",
    "1,Alice,\"Likes, commas\"\n",
    "2,Bob,Plain text\n",
    "\"\n",
    "print(pd.read_csv(StringIO(raw)))\n",
    "print('\n",
    "‡∏Å‡∏≥‡∏´‡∏ô‡∏î quotechar ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á:')\n",
    "print(pd.read_csv(StringIO(raw), quotechar='\"'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5db812",
   "metadata": {},
   "source": [
    "### File vs DB ‚Äî ‡πÉ‡∏ä‡πâ‡∏ï‡∏≠‡∏ô‡πÑ‡∏´‡∏ô? (Comparison Strategy)\n",
    "\n",
    "![File vs DB Decision](images/file_vs_db_decision.png)\n",
    "\n",
    "| Feature | File System (Parquet/CSV) | Database (MySQL/Postgres) |\n",
    "|---|---|---|\n",
    "| **Primary Goal** | Analytics / Batch Processing (‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏¢‡∏≠‡∏∞ ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß) | Transactional / Real-time Updates (‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ï‡∏•‡∏≠‡∏î‡πÄ‡∏ß‡∏•‡∏≤) |\n",
    "| **Cost** | ‡∏ñ‡∏π‡∏Å (S3/HDFS) | ‡πÅ‡∏û‡∏á (Server Resource/License) |\n",
    "| **Update** | ‡∏¢‡∏≤‡∏Å (‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ó‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡πÑ‡∏ü‡∏•‡πå) | ‡∏á‡πà‡∏≤‡∏¢ (Update ‡∏ó‡∏µ‡∏•‡∏∞ row ‡πÑ‡∏î‡πâ) |\n",
    "| **ACID** | ‡πÑ‡∏°‡πà‡∏°‡∏µ (‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ Lakehouse ‡∏ä‡πà‡∏ß‡∏¢) | ‡∏°‡∏µ‡∏Ñ‡∏£‡∏ö (‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908c24a5",
   "metadata": {},
   "source": [
    "## Part 2: The Bottleneck ‚Äî Stack vs Heap\n",
    "- **Stack**: ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç/‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÄ‡∏•‡πá‡∏Å + ‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ (reference) ‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á\n",
    "- **Heap**: ‡πÄ‡∏Å‡πá‡∏ö‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏à‡∏£‡∏¥‡∏á (list, DataFrame) ‡∏Å‡∏¥‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏¢‡∏≠‡∏∞\n",
    "- **Garbage Collector**: ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡∏≠‡∏á‡πÉ‡∏ô Heap ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ reference ‡∏ä‡∏µ‡πâ‡∏ñ‡∏∂‡∏á\n",
    "- Analogy: ‡∏£‡∏µ‡πÇ‡∏°‡∏ó (stack) ‚Üî ‡∏ó‡∏µ‡∏ß‡∏µ (heap)\n",
    "\n",
    "![Stack vs Heap](images/stack_vs_heap.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bd166f",
   "metadata": {},
   "source": [
    "### Copy vs View (‡∏ó‡∏≥‡πÑ‡∏° RAM ‡∏û‡∏∏‡πà‡∏á)\n",
    "- pandas/numpy ‡∏ö‡∏≤‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÉ‡∏´‡πâ **view** (‡∏ä‡∏µ‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏°) ‚Üí ‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î RAM\n",
    "- ‡πÅ‡∏ï‡πà‡∏ö‡∏≤‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á **copy** ‡∏à‡∏£‡∏¥‡∏á ‚Üí RAM ‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ó‡∏±‡∏ô‡∏ó‡∏µ\n",
    "\n",
    "![Reference vs Copy](images/reference_vs_copy.png)\n",
    "- ‡∏£‡∏π‡πâ‡∏ó‡∏±‡∏ô‡∏î‡πâ‡∏ß‡∏¢ `.copy()` ‡πÅ‡∏•‡∏∞‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï memory usage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b324fe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "arr = np.arange(10)\n",
    "view = arr[2:6]   # view\n",
    "view[0] = 999\n",
    "print('arr:', arr)  # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ï‡∏≤‡∏°‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÅ‡∏ä‡∏£‡πå memory\n",
    "\n",
    "# copy = arr[2:6].copy()\n",
    "# copy[0] = 555\n",
    "# print('arr after copy change:', arr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9368e32",
   "metadata": {},
   "source": [
    "### Garbage Collection (GC) & Memory Leak ‚Äî ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà Big Data Engineer ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏π‡πâ\n",
    "\n",
    "**1. Garbage Collection (GC)**:\n",
    "- ‡∏Ñ‡∏∑‡∏≠ \"‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î\" ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ (‡πÉ‡∏ô Java/Python) ‡∏ó‡∏µ‡πà‡∏Ñ‡∏≠‡∏¢‡πÄ‡∏î‡∏¥‡∏ô‡πÄ‡∏Å‡πá‡∏ö‡∏Å‡∏ß‡∏≤‡∏î‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥ (RAM) ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏Ñ‡∏£‡πÉ‡∏ä‡πâ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡∏∑‡∏ô‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö\n",
    "- ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ GC: ‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏à‡∏∞‡∏Å‡∏¥‡∏ô RAM ‡πÑ‡∏õ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏¢‡πÜ ‡∏à‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Ñ‡πâ‡∏≤‡∏á\n",
    "\n",
    "**2. Memory Leak**:\n",
    "- ‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤ \"‡πÄ‡∏ú‡∏•‡∏≠‡∏ú‡∏π‡∏Å‡πÄ‡∏ä‡∏∑‡∏≠‡∏Å\" (Reference) ‡πÑ‡∏ß‡πâ‡∏Å‡∏±‡∏ö‡∏Ç‡∏¢‡∏∞ ‡∏ó‡∏≥‡πÉ‡∏´‡πâ GC ‡πÑ‡∏°‡πà‡∏Å‡∏•‡πâ‡∏≤‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏õ‡∏ó‡∏¥‡πâ‡∏á ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ô‡∏∂‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏£‡∏≤‡∏¢‡∏±‡∏á‡πÉ‡∏ä‡πâ‡∏≠‡∏¢‡∏π‡πà\n",
    "- **‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡πÉ‡∏ô Big Data**: Spark Driver/Executor ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡∏ï‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ error `OutOfMemory` ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ Code ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô dataframe ‡πÄ‡∏Å‡πá‡∏ö‡πÉ‡∏™‡πà list ‡πÑ‡∏ß‡πâ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏¢‡πÜ ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏•‡∏µ‡∏¢‡∏£‡πå‡∏ó‡∏¥‡πâ‡∏á\n",
    "\n",
    "![GC & Memory Leak](images/gc_memory_leak.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18306f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: reference vs copy\n",
    "import sys, copy\n",
    "\n",
    "a = [1, 2, 3]\n",
    "b = a              # ‡πÅ‡∏ä‡∏£‡πå‡∏£‡∏µ‡πÇ‡∏°‡∏ó\n",
    "c = copy.deepcopy(a)  # ‡∏™‡∏≥‡πÄ‡∏ô‡∏≤‡∏Ç‡∏≠‡∏á‡∏à‡∏£‡∏¥‡∏á\n",
    "\n",
    "print('id(a)', id(a), 'id(b)', id(b), 'id(c)', id(c))\n",
    "a.append(99)\n",
    "print('‡∏´‡∏•‡∏±‡∏á append ‡∏ó‡∏µ‡πà a:')\n",
    "print('a =', a)\n",
    "print('b =', b, '<-- ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ï‡∏≤‡∏°‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÅ‡∏ä‡∏£‡πå reference')\n",
    "print('c =', c, '<-- ‡πÑ‡∏°‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏û‡∏£‡∏≤‡∏∞ copy ‡∏Ç‡∏≠‡∏á‡∏à‡∏£‡∏¥‡∏á')\n",
    "print('‡∏Ç‡∏ô‡∏≤‡∏î list a (bytes):', sys.getsizeof(a))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f933c6da",
   "metadata": {},
   "source": [
    "### Intro to Pandas & DataFrame (‡∏õ‡∏π‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô)\n",
    "- **Pandas**: ‡∏Ñ‡∏∑‡∏≠ Library ‡∏™‡∏≤‡∏°‡∏±‡∏ç‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏ö‡πâ‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á Python ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏£‡∏≤‡∏á (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô Excel ‡∏ï‡∏¥‡∏î‡∏õ‡∏µ‡∏Å)\n",
    "- **DataFrame**: ‡∏Ñ‡∏∑‡∏≠ Object ‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á Pandas ‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡∏≤‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á ‡∏°‡∏µ Row Index ‡πÅ‡∏•‡∏∞ Column Name\n",
    "- ‡πÅ‡∏ï‡πà‡∏£‡∏∞‡∏ß‡∏±‡∏á! DataFrame ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏á RAM (In-memory) ‡∏ñ‡πâ‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà‡∏Å‡∏ß‡πà‡∏≤ RAM ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á -> ‡∏à‡∏ö‡πÄ‡∏´‡πà (‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ Spark/Dask ‡πÅ‡∏ó‡∏ô)\n",
    "\n",
    "### Memory Footprint ‡∏Ç‡∏≠‡∏á DataFrame ‚Äî ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å dtype ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞\n",
    "- ‡∏Ñ‡πà‡∏≤ `int64/float64` ‡∏´‡∏ô‡∏±‡∏Å‡∏Å‡∏ß‡πà‡∏≤ `int32/float32`\n",
    "- ‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏ã‡πâ‡∏≥‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ `category` ‚Üí ‡∏•‡∏î RAM ‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Å‡∏±‡∏ö Parquet dictionary ‡πÑ‡∏î‡πâ‡∏î‡∏µ\n",
    "- ‡∏ä‡∏≠‡∏ö‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: \"‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ô‡∏µ‡πâ‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏∞‡πÑ‡∏£‡∏¢‡∏≤‡∏ß‡∏™‡∏∏‡∏î‡πÑ‡∏´‡∏°?\" ‡πÄ‡∏ä‡πà‡∏ô `user_id` 0-1e6 ‡∏û‡∏≠‡πÉ‡∏ä‡πâ `int32`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66af50c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "rows = 200_000\n",
    "raw_df = pd.DataFrame({\n",
    "    'user_id': np.random.randint(0, 1_000_000, rows),\n",
    "    'country': np.random.choice(['TH','US','JP','DE','FR','UK'], rows),\n",
    "    'value': np.random.randn(rows)\n",
    "})\n",
    "\n",
    "print('default dtypes:', raw_df.dtypes)\n",
    "print('memory (MB):', raw_df.memory_usage(deep=True).sum() / 1e6)\n",
    "\n",
    "optimized = raw_df.assign(\n",
    "    user_id = raw_df['user_id'].astype('int32'),\n",
    "    country = raw_df['country'].astype('category'),\n",
    "    value = raw_df['value'].astype('float32')\n",
    ")\n",
    "print('\\noptimized dtypes:', optimized.dtypes)\n",
    "print('memory (MB):', optimized.memory_usage(deep=True).sum() / 1e6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d000b7",
   "metadata": {},
   "source": [
    "### Data Architecture Evolution: Warehouse vs Lake vs Lakehouse\n",
    "\n",
    "1. **Data Warehouse (‡∏¢‡∏∏‡∏Ñ 1990s)**: \n",
    "   - ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á (Table) \n",
    "   - ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ: ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö, ‡πÄ‡∏£‡πá‡∏ß, ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠‡πÑ‡∏î‡πâ \n",
    "   - ‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢: ‡πÅ‡∏û‡∏á, ‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏†‡∏≤‡∏û/‡πÄ‡∏™‡∏µ‡∏¢‡∏á/Video\n",
    "\n",
    "2. **Data Lake (‡∏¢‡∏∏‡∏Ñ 2010s)**: \n",
    "   - ‡∏ó‡∏∞‡πÄ‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÄ‡∏ó‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏á‡πÑ‡∏õ (Files) \n",
    "   - ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ: ‡∏ñ‡∏π‡∏Å, ‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏î‡πâ‡∏ó‡∏∏‡∏Å‡∏ó‡πà‡∏≤ \n",
    "   - ‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢: ‡πÄ‡∏•‡∏∞‡πÄ‡∏ó‡∏∞ (Data Swamp), ‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Transaction (ACID), Update ‡∏¢‡∏≤‡∏Å\n",
    "\n",
    "3. **Data Lakehouse (‡∏¢‡∏∏‡∏Ñ 2020s)**: \n",
    "   - ‡∏•‡∏π‡∏Å‡∏ú‡∏™‡∏°: ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô Lake (‡∏ñ‡∏π‡∏Å) + ‡∏°‡∏µ Metadata Log ‡∏Ñ‡∏∏‡∏° (ACID, Schema) \n",
    "   - ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ `UPDATE/DELETE` ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå Parquet ‡πÑ‡∏î‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏ó‡∏≥‡πÉ‡∏ô Database!\n",
    "\n",
    "| ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥ | Warehouse | Lake | Lakehouse (Delta/Iceberg/Hudi) |\n",
    "|---|---|---|---|\n",
    "| Schema enforcement | ‡∏™‡∏π‡∏á | ‡∏ï‡πà‡∏≥ | ‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á/‡∏™‡∏π‡∏á |\n",
    "| ACID | ‚úÖ | ‚ùå | ‚úÖ (‡∏ú‡πà‡∏≤‡∏ô log + manifest) |\n",
    "| Format ‡∏´‡∏•‡∏±‡∏Å | Proprietary | Any File | Open Format (Parquet) + Log |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7659d2a",
   "metadata": {},
   "source": [
    "## Part 3: Modern Storage Wars ‚Äî Hands-on\n",
    "‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö CSV (row-based) vs Parquet (columnar)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f608646c",
   "metadata": {},
   "source": [
    "### Format Zoo ‚Äî CSV/JSON/Avro/Parquet/ORC\n",
    "| Format | ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á | ‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô | ‡∏à‡∏∏‡∏î‡∏≠‡πà‡∏≠‡∏ô |\n",
    "|---|---|---|---|\n",
    "| CSV | Row | ‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢, ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏ó‡∏∏‡∏Å‡∏ó‡∏µ‡πà | ‡πÑ‡∏°‡πà‡∏°‡∏µ schema, ‡πÉ‡∏´‡∏ç‡πà, ‡∏ä‡πâ‡∏≤ |\n",
    "| JSON | Row/Nested | ‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô, nested ‡πÑ‡∏î‡πâ | ‡πÉ‡∏´‡∏ç‡πà, parsing ‡∏ä‡πâ‡∏≤ |\n",
    "| Avro | Row + Schema | schema ‡∏ä‡∏±‡∏î, write fast | ‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö column pruning |\n",
    "| Parquet | Columnar | ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏£‡πá‡∏ß, compression ‡∏î‡∏µ | write ‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤ row format |\n",
    "| ORC | Columnar | ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î, compact | tooling ‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤ Parquet |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bb7717",
   "metadata": {},
   "source": [
    "### Row vs Columnar ‚Äî ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏Å‡πá‡∏ö‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£\n",
    "- **Row (CSV/JSON)**: ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô/‡∏≠‡πà‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡πÅ‡∏ñ‡∏ß‡∏£‡∏ß‡∏î‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‚Üí ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö workload ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡πà‡∏≤‡∏ô‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå\n",
    "- **Columnar (Parquet/ORC)**: ‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå ‚Üí ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ (column pruning), ‡∏ö‡∏µ‡∏ö‡∏≠‡∏±‡∏î‡πÑ‡∏î‡πâ‡∏™‡∏π‡∏á, ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö predicate pushdown\n",
    "- ‡∏à‡∏∏‡∏î‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à: ‡∏ñ‡πâ‡∏≤ query ‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏°‡πà‡∏Å‡∏µ‡πà‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏µ filter ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‚Üí columnar ‡∏ä‡∏ô‡∏∞; ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á append record ‡πÅ‡∏ö‡∏ö real-time ‡πÅ‡∏•‡∏∞‡∏≠‡πà‡∏≤‡∏ô‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå ‚Üí row-format ‡∏≠‡∏≤‡∏à‡∏û‡∏≠‡πÄ‡∏û‡∏µ‡∏¢‡∏á\n",
    "\n",
    "![Row vs Columnar](images/row_vs_columnar.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14ee482",
   "metadata": {},
   "source": [
    "### ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á Column Pruning ‡πÅ‡∏ö‡∏ö‡∏†‡∏≤‡∏û‡∏à‡∏≥\n",
    "‡∏°‡∏µ‡∏ï‡∏≤‡∏£‡∏≤‡∏á 5 ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå ‡πÅ‡∏ï‡πà query ‡πÉ‡∏ä‡πâ‡πÅ‡∏Ñ‡πà 2 ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå: columnar ‡∏à‡∏∞‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞ 2 ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ô‡∏±‡πâ‡∏ô\n",
    "‚Üí ‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î I/O ‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡∏°‡∏≤‡∏Å ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå\n",
    "\n",
    "![Column Pruning](images/column_pruning.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf66ff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, time, duckdb\n",
    "np.random.seed(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611ba3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≥‡∏•‡∏≠‡∏á\n",
    "\n",
    "def generate_data(rows=1_000_000):\n",
    "    df = pd.DataFrame({\n",
    "        'id': np.arange(rows),\n",
    "        'category': np.random.choice(['Electronic', 'Clothing', 'Furniture', 'Food'], rows),\n",
    "        'status': np.random.choice(['Completed', 'Pending', 'Failed'], rows),\n",
    "        'value': np.random.randn(rows),\n",
    "        'description': np.random.choice(['Data for big data class'] * 5, rows)\n",
    "    })\n",
    "    return df\n",
    "\n",
    "%time df = generate_data()\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4126f25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save & ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå\n",
    "%time df.to_csv('data.csv', index=False)\n",
    "%time df.to_parquet('data.parquet', index=False)\n",
    "\n",
    "csv_mb = os.path.getsize('data.csv') / (1024 * 1024)\n",
    "parquet_mb = os.path.getsize('data.parquet') / (1024 * 1024)\n",
    "\n",
    "print(f\"CSV Size: {csv_mb:.2f} MB\")\n",
    "print(f\"Parquet Size: {parquet_mb:.2f} MB\")\n",
    "print(f\"‡πÄ‡∏•‡πá‡∏Å‡∏•‡∏á‡∏Å‡∏µ‡πà‡πÄ‡∏ó‡πà‡∏≤: {csv_mb / parquet_mb:.1f}x\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee41dc5a",
   "metadata": {},
   "source": [
    "### Read Only Needed Columns (pyarrow)\n",
    "‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏à‡∏£‡∏¥‡∏á ‚Üí ‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ RAM ‡∏ô‡πâ‡∏≠‡∏¢‡∏•‡∏á\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089f4ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "pf = pq.ParquetFile('data.parquet')\n",
    "# ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ\n",
    "subset = pf.read(columns=['status', 'value']).to_pandas()\n",
    "subset.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7acc22",
   "metadata": {},
   "source": [
    "### Row Group Size Experiment (Deep Dive)\n",
    "Row Group ‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡πà‡∏á‡πÑ‡∏ü‡∏•‡πå Parquet ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡πâ‡∏≠‡∏ô‡∏¢‡πà‡∏≠‡∏¢ ‡πÜ ‡πÉ‡∏ô‡πÅ‡∏ô‡∏ß‡∏ô‡∏≠‡∏ô (Horizontal Partitioning ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå)\n",
    "\n",
    "**Trade-off ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏π‡πâ:**\n",
    "1. **Row Group ‡πÉ‡∏´‡∏ç‡πà (‡πÄ‡∏ä‡πà‡∏ô 512MB - 1GB):**\n",
    "   - ‚úÖ Compression ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡∏Å (‡πÄ‡∏´‡πá‡∏ô pattern ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏¢‡∏≠‡∏∞‡∏Ç‡∏∂‡πâ‡∏ô)\n",
    "   - ‚úÖ Metadata ‡∏ô‡πâ‡∏≠‡∏¢‡∏•‡∏á (File footer ‡πÄ‡∏•‡πá‡∏Å)\n",
    "   - ‚ùå ‡∏≠‡πà‡∏≤‡∏ô‡∏ä‡πâ‡∏≤‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏Ñ‡πà \"‡∏ö‡∏≤‡∏á‡πÅ‡∏ñ‡∏ß\" (‡∏ï‡πâ‡∏≠‡∏á‡πÇ‡∏´‡∏•‡∏î‡∏°‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏Å‡πâ‡∏≠‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£)\n",
    "\n",
    "2. **Row Group ‡πÄ‡∏•‡πá‡∏Å (‡πÄ‡∏ä‡πà‡∏ô 1MB - 10MB):**\n",
    "   - ‚úÖ Pruning ‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ (‡∏Ç‡πâ‡∏≤‡∏°‡∏Å‡πâ‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡πÑ‡∏î‡πâ‡πÄ‡∏¢‡∏≠‡∏∞)\n",
    "   - ‚ùå Metadata ‡∏ö‡∏ß‡∏° (Header/Footer ‡∏Å‡∏¥‡∏ô‡∏ó‡∏µ‡πà)\n",
    "   - ‚ùå Compression ‡πÅ‡∏¢‡πà‡∏•‡∏á\n",
    "\n",
    "**Best Practice:** ‡πÉ‡∏ä‡πâ default (~128MB) ‡∏´‡∏£‡∏∑‡∏≠‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏Å‡∏≤‡∏£ Query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5498dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa, pyarrow.parquet as pq, os, time\n",
    "\n",
    "table = pa.Table.from_pandas(df)\n",
    "# ‡∏•‡∏≠‡∏á save ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Row Group ‡πÄ‡∏•‡πá‡∏Å vs ‡πÉ‡∏´‡∏ç‡πà\n",
    "for rg in [5000, 1_000_000]:\n",
    "    fn = f\"rg_{rg}.parquet\"\n",
    "    pq.write_table(table, fn, row_group_size=rg)\n",
    "    size_mb = os.path.getsize(fn)/(1024*1024)\n",
    "    print(f\"Row Group Size: {rg:<10} File Size: {size_mb:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e0a3d5",
   "metadata": {},
   "source": [
    "### Compression Shootout (Snappy vs Gzip vs Zstd)\n",
    "‡∏•‡∏≠‡∏á‡∏ö‡∏µ‡∏ö‡∏≠‡∏±‡∏î Parquet ‡∏î‡πâ‡∏ß‡∏¢ codec ‡∏ï‡πà‡∏≤‡∏á ‡πÜ ‡πÅ‡∏•‡πâ‡∏ß‡∏î‡∏π‡∏Ç‡∏ô‡∏≤‡∏î/‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô\n",
    "- Snappy: ‡πÄ‡∏£‡πá‡∏ß, ‡∏Ç‡∏ô‡∏≤‡∏î‡∏û‡∏≠‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì (‡πÄ‡∏´‡∏°‡∏≤‡∏∞ default)\n",
    "- Gzip: ‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤ ‡πÅ‡∏ï‡πà‡πÄ‡∏•‡πá‡∏Å‡∏•‡∏á‡∏≠‡∏µ‡∏Å\n",
    "- Zstd: ‡∏™‡∏°‡∏î‡∏∏‡∏• ‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å‡∏Å‡∏ß‡πà‡∏≤ Snappy ‡πÅ‡∏ï‡πà‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤ Gzip\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a369da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, os\n",
    "import pandas as pd\n",
    "\n",
    "codecs = ['snappy','gzip','zstd','none']\n",
    "rows = 400_000\n",
    "cdf = df.sample(rows, replace=False).reset_index(drop=True)\n",
    "\n",
    "results = []\n",
    "for codec in codecs:\n",
    "    fn = f\"data_{codec}.parquet\"\n",
    "    start = time.time()\n",
    "    cdf.to_parquet(fn, compression=None if codec=='none' else codec, index=False)\n",
    "    elapsed = time.time() - start\n",
    "    size_mb = os.path.getsize(fn)/(1024*1024)\n",
    "    results.append((codec, size_mb, elapsed))\n",
    "\n",
    "pd.DataFrame(results, columns=['codec','size_mb','write_sec']).sort_values('size_mb')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2b1d43",
   "metadata": {},
   "source": [
    "### Parquet Internals ‡πÅ‡∏ö‡∏ö‡∏™‡∏±‡πâ‡∏ô‡πÜ\n",
    "- ‡πÑ‡∏ü‡∏•‡πå‡∏ñ‡∏π‡∏Å‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô **Row Groups** (‡∏°‡∏±‡∏Å ~128MB) ‚Üí ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏≠‡πà‡∏≤‡∏ô/‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏î‡πâ\n",
    "- ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏ô row group ‡∏°‡∏µ **Dictionary** (‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡∏¥‡∏î) + **min/max stats** + optional **Bloom filter**\n",
    "- ‡πÄ‡∏ß‡∏•‡∏≤ query: engine ‡∏î‡∏π metadata ‡∏Å‡πà‡∏≠‡∏ô ‚Üí ‡∏ï‡∏±‡∏î row group ‡∏ó‡∏µ‡πà‡∏Ñ‡πà‡∏≤ min/max ‡πÑ‡∏°‡πà‡∏ó‡∏±‡∏ö‡∏Å‡∏±‡∏ö‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç (row-group pruning) ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏≠‡∏¢‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ\n",
    "- Stats ‡∏¢‡∏¥‡πà‡∏á‡πÅ‡∏°‡πà‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ñ‡∏π‡∏Å sort/cluster ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏°‡∏±‡∏Å filter\n",
    "\n",
    "![Parquet Structure](images/parquet_structure.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87224f2f",
   "metadata": {},
   "source": [
    "### ‡∏ó‡∏≥‡πÑ‡∏° Parquet ‡∏ñ‡∏∂‡∏á‡πÄ‡∏•‡πá‡∏Å‡πÅ‡∏•‡∏∞‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏£‡πá‡∏ß?\n",
    "- **Dictionary Encoding**: ‡πÄ‡∏Å‡πá‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á mapping ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏™‡∏±‡πâ‡∏ô ‡πÜ\n",
    "- **Run-Length Encoding (RLE)**: ‡∏Ñ‡πà‡∏≤‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô‡∏¢‡∏≤‡∏ß ‡πÜ ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏õ‡πá‡∏ô ‚Äú‡∏Ñ‡πà‡∏≤ √ó ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‚Äù\n",
    "- **Columnar Layout**: ‡πÄ‡∏Å‡πá‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå ‚Üí ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ (column pruning)\n",
    "- **Predicate Pushdown**: ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞ row group ‡∏ó‡∏µ‡πà‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏ï‡∏£‡∏á (‡πÄ‡∏ä‡πà‡∏ô status = 'Completed')\n",
    "\n",
    "![Encoding Techniques](images/encoding_techniques.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b42fe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: column pruning & predicate pushdown ‡∏î‡πâ‡∏ß‡∏¢ DuckDB\n",
    "# DuckDB ‡∏à‡∏∞‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á load ‡∏ó‡∏±‡πâ‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Ç‡πâ‡∏≤‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥‡∏Å‡πà‡∏≠‡∏ô)\n",
    "\n",
    "# ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞ 2 ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå\n",
    "%time duckdb.query(\"SELECT category, status FROM 'data.csv' LIMIT 5\").df()\n",
    "%time duckdb.query(\"SELECT category, status FROM 'data.parquet' LIMIT 5\").df()\n",
    "\n",
    "# ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ Completed (predicate pushdown ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Å‡∏±‡∏ö Parquet)\n",
    "%time duckdb.query(\"SELECT count(*) FROM 'data.csv' WHERE status='Completed'\").df()\n",
    "%time duckdb.query(\"SELECT count(*) FROM 'data.parquet' WHERE status='Completed'\").df()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f117963",
   "metadata": {},
   "source": [
    "### Read Speed Compare ‚Äî DuckDB vs pandas\n",
    "- DuckDB ‡∏≠‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á ‡πÉ‡∏ä‡πâ predicate/column pruning ‡πÄ‡∏ï‡πá‡∏°‡∏ó‡∏µ‡πà\n",
    "- pandas ‡∏ï‡πâ‡∏≠‡∏á‡πÇ‡∏´‡∏•‡∏î‡∏ó‡∏±‡πâ‡∏á‡πÑ‡∏ü‡∏•‡πå (‡πÅ‡∏ï‡πà‡∏Å‡∏±‡∏ö Parquet ‡∏¢‡∏±‡∏á‡πÑ‡∏î‡πâ column pruning ‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94ccaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, time\n",
    "\n",
    "start = time.time(); pd.read_csv('data.csv', usecols=['status']).query(\"status=='Completed'\").shape[0]; csv_time = time.time()-start\n",
    "start = time.time(); pd.read_parquet('data.parquet', columns=['status']).query(\"status=='Completed'\").shape[0]; pq_time = time.time()-start\n",
    "\n",
    "print(f\"pandas CSV count took {csv_time:.2f}s; Parquet took {pq_time:.2f}s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639f199e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect Parquet metadata (row groups + stats) ‡∏î‡πâ‡∏ß‡∏¢ PyArrow\n",
    "import pyarrow.parquet as pq\n",
    "pf = pq.ParquetFile('data.parquet')\n",
    "print('Row groups:', pf.num_row_groups)\n",
    "print('Schema:')\n",
    "print(pf.schema)\n",
    "\n",
    "# ‡∏î‡∏π‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå status ‡πÉ‡∏ô row group ‡πÅ‡∏£‡∏Å\n",
    "rg0 = pf.metadata.row_group(0).column(pf.schema.names.index('status'))\n",
    "print('status stats rowgroup0:', rg0.statistics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d8e9e6",
   "metadata": {},
   "source": [
    "### Data Quality Check Mini-lab\n",
    "- ‡∏ï‡∏£‡∏ß‡∏à missing values / duplicate IDs\n",
    "- ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì % missing ‡πÅ‡∏•‡πâ‡∏ß‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏•‡∏ö ‡πÄ‡∏ï‡∏¥‡∏° ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e5cc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ï‡∏£‡∏ß‡∏à‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô\n",
    "print('missing per column:')\n",
    "print(df.isna().mean())\n",
    "print('duplicate id count:', df['id'].duplicated().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd71c88",
   "metadata": {},
   "source": [
    "### Activity 2 ‚Äî Cardinality & Compression\n",
    "‡∏•‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô category/status ‡πÉ‡∏´‡πâ‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô ‡πÅ‡∏•‡πâ‡∏ß‡∏ß‡∏±‡∏î‡∏Ç‡∏ô‡∏≤‡∏î Parquet ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö CSV\n",
    "- ‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á: ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Ñ‡πà‡∏≤‡∏ã‡πâ‡∏≥‡∏•‡∏î‡∏•‡∏á (cardinality ‡∏™‡∏π‡∏á) ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Å‡∏≤‡∏£‡∏ö‡∏µ‡∏ö‡∏≠‡∏±‡∏î‡∏à‡∏∞‡∏•‡∏î‡∏•‡∏á\n",
    "- ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á rows = 200_000, 1_000_000 ‡πÅ‡∏•‡∏∞ cardinality ‡∏ï‡πà‡∏≤‡∏á ‡πÜ\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50856fc2",
   "metadata": {},
   "source": [
    "#### Partitioning: ‡∏ó‡∏≥‡πÅ‡∏ö‡∏ö‡πÑ‡∏´‡∏ô‡∏î‡∏µ?\n",
    "- ‡∏î‡∏µ: `year/month/day` ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤ ‚Üí filter ‡∏á‡πà‡∏≤‡∏¢\n",
    "- ‡∏£‡∏∞‡∏ß‡∏±‡∏á: partition ‡∏ï‡∏≤‡∏° `user_id` ‡∏´‡∏£‡∏∑‡∏≠ `country` ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏°‡∏≤‡∏Å ‚Üí ‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏•‡πá‡∏Å‡∏ô‡∏±‡∏ö‡∏´‡∏°‡∏∑‡πà‡∏ô\n",
    "- ‡πÉ‡∏ä‡πâ `repartition` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏∏‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÑ‡∏ü‡∏•‡πå ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏Ç‡∏ô‡∏≤‡∏î 128‚Äì512MB/‡πÑ‡∏ü‡∏•‡πå\n",
    "- ‡∏´‡∏•‡∏±‡∏á append ‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏£‡∏°‡∏µ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô **compaction** ‡∏£‡∏ß‡∏°‡πÑ‡∏ü‡∏•‡πå\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbebf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: cardinality vs size (Parquet vs CSV)\n",
    "import pandas as pd, numpy as np, os, time\n",
    "\n",
    "rows = 300_000\n",
    "cardinalities = [4, 20, 200, 1000]\n",
    "results = []\n",
    "\n",
    "for card in cardinalities:\n",
    "    categories = [f\"cat_{i}\" for i in range(card)]\n",
    "    experiment_experiment_experiment_experiment_df = pd.DataFrame({\n",
    "        'id': np.arange(rows),\n",
    "        'cat': np.random.choice(categories, rows),\n",
    "        'value': np.random.randn(rows)\n",
    "    })\n",
    "    csv_path = f\"card_{card}.csv\"\n",
    "    pq_path = f\"card_{card}.parquet\"\n",
    "    experiment_experiment_df.to_csv(csv_path, index=False)\n",
    "    experiment_experiment_df.to_parquet(pq_path, index=False)\n",
    "    csv_mb = os.path.getsize(csv_path)/(1024*1024)\n",
    "    pq_mb = os.path.getsize(pq_path)/(1024*1024)\n",
    "    results.append((card, csv_mb, pq_mb, csv_mb/pq_mb))\n",
    "\n",
    "pd.DataFrame(results, columns=['cardinality','csv_mb','parquet_mb','ratio_csv_div_parquet']).sort_values('cardinality')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133bdd80",
   "metadata": {},
   "source": [
    "### Schema Evolution Mini-lab\n",
    "- ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏´‡∏°‡πà (‡πÄ‡∏ä‡πà‡∏ô `discount`) ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ó‡∏±‡∏ö‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå Parquet ‡πÄ‡∏î‡∏¥‡∏°\n",
    "- ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà‡∏°‡∏µ schema ‡∏ï‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Å‡πà‡∏≤ ‚Üí query engine ‡∏ï‡πâ‡∏≠‡∏á union schema\n",
    "- DuckDB / Spark ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ union schema ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏ñ‡πâ‡∏≤‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡∏¥‡∏î\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d77586",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3338b70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏´‡∏°‡πà‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ó‡∏±‡∏ö‡∏ö‡∏≤‡∏á partition\n",
    "import pandas as pd\n",
    "new_df = df.copy()\n",
    "new_df['discount'] = np.random.randint(0, 30, len(new_df))\n",
    "\n",
    "duckdb.query(\"COPY (SELECT * FROM new_df) TO 'lakehouse' (FORMAT 'parquet', PARTITION_BY ['cat'])\")\n",
    "\n",
    "# ‡∏≠‡πà‡∏≤‡∏ô‡∏£‡∏ß‡∏° schema\n",
    "duckdb.query(\"SELECT count(*), avg(discount) FROM 'lakehouse'\").df().head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02575bb",
   "metadata": {},
   "source": [
    "### Quick Check (‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Ñ‡∏≤‡∏ö)\n",
    "1) ‡∏ó‡∏≥‡πÑ‡∏° NameNode/S3 list ‡∏´‡∏•‡∏≤‡∏¢‡∏û‡∏±‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ñ‡∏∂‡∏á‡∏ä‡πâ‡∏≤ ‡πÅ‡∏°‡πâ‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏∞‡πÄ‡∏•‡πá‡∏Å‡∏°‡∏≤‡∏Å?  \n",
    "2) ‡∏ñ‡πâ‡∏≤ query ‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÅ‡∏Ñ‡πà 3 ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏à‡∏≤‡∏Å 200 ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå ‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ü‡∏≠‡∏£‡πå‡πÅ‡∏°‡∏ï‡∏≠‡∏∞‡πÑ‡∏£? ‡∏ó‡∏≥‡πÑ‡∏°?  \n",
    "3) ‡∏à‡∏∞‡∏ï‡∏±‡πâ‡∏á partition ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á log ‡∏ï‡πà‡∏≠‡∏ß‡∏±‡∏ô‡πÉ‡∏´‡πâ query `WHERE date BETWEEN ...` ‡πÄ‡∏£‡πá‡∏ß ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏£‡∏∞‡πÄ‡∏ö‡∏¥‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÑ‡∏ü‡∏•‡πå?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fdeddf",
   "metadata": {},
   "source": [
    "### Activity 3 ‚Äî Mini Lakehouse (Partition & Query)\n",
    "1. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå `lakehouse/` ‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô Parquet ‡πÅ‡∏ö‡∏ö partition ‡∏ï‡∏≤‡∏° `status`\n",
    "2. ‡πÉ‡∏ä‡πâ DuckDB ‡∏≠‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏ä‡πà‡∏ô `SELECT status, COUNT(*) FROM 'lakehouse' GROUP BY 1`\n",
    "3. ‡∏•‡∏≠‡∏á query ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ `status='Failed'` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡πÄ‡∏ß‡∏•‡∏≤ (partition pruning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76c4449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build mini lakehouse partitioned by status\n",
    "import os, duckdb, shutil\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# CLEANUP: Remove directory if it exists to avoid IOException\n",
    "if os.path.exists('lakehouse'):\n",
    "    shutil.rmtree('lakehouse')\n",
    "os.makedirs('lakehouse', exist_ok=True)\n",
    "\n",
    "# Use explicit connection to ensure variable visibility\n",
    "con = duckdb.connect()\n",
    "con.register('df_view', df)\n",
    "\n",
    "# ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÅ‡∏ö‡∏ö partition\n",
    "con.execute(\"COPY (SELECT * FROM df_view) TO 'lakehouse' (FORMAT 'parquet', PARTITION_BY (status))\")\n",
    "\n",
    "# Query ‡∏ó‡∏±‡πâ‡∏á‡∏Å‡πâ‡∏≠‡∏ô (use glob pattern to find files)\n",
    "%time con.execute(\"SELECT status, COUNT(*) AS cnt, AVG(value) AS avg_value FROM 'lakehouse/**/*.parquet' GROUP BY 1\").df()\n",
    "\n",
    "# Query ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Failed (‡∏à‡∏∞ prune partition ‡∏≠‡∏∑‡πà‡∏ô)\n",
    "%time con.execute(\"SELECT * FROM 'lakehouse/**/*.parquet' WHERE status='Failed' LIMIT 5\").df()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402c8dc6",
   "metadata": {},
   "source": [
    "### üí• Experiment: The \"Small Files\" Nightmare (Bad Partitioning)\n",
    "‡∏´‡πâ‡∏≤‡∏° Partition ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô‡πÄ‡∏¢‡∏≠‡∏∞ ‡πÜ (High Cardinality) ‡πÄ‡∏ä‡πà‡∏ô `id` ‡∏´‡∏£‡∏∑‡∏≠ `timestamp` ‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ\n",
    "‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏à‡∏∞‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏Å‡∏¥‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏¢‡∏∞‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏´‡∏≤‡∏®‡∏≤‡∏• (Small Files Problem) ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏•‡πà‡∏°‡πÑ‡∏î‡πâ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beb2f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: ‡∏´‡∏≤‡∏¢‡∏ô‡∏∞‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£ Partition ‡∏ú‡∏¥‡∏î (‡∏≠‡∏¢‡πà‡∏≤‡∏´‡∏≤‡∏ó‡∏≥‡πÉ‡∏ô Production!)\n",
    "import shutil, os\n",
    "import duckdb\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á high cardinality dataframe\n",
    "bad_df = df.copy()\n",
    "bad_df['many_partitions'] = bad_df['id'] % 2000  # ‡∏™‡∏£‡πâ‡∏≤‡∏á 2,000 partitions\n",
    "\n",
    "output_dir = 'bad_partitioning'\n",
    "if os.path.exists(output_dir): shutil.rmtree(output_dir)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print('Starting bad partition write...')\n",
    "# Use explicit connection and register table to prevent BinderException\n",
    "con = duckdb.connect()\n",
    "con.register('bad_df_view', bad_df)\n",
    "%time con.execute(\"COPY (SELECT * FROM bad_df_view) TO 'bad_partitioning' (FORMAT 'parquet', PARTITION_BY (many_partitions))\")\n",
    "\n",
    "num_files = sum([len(files) for r, d, files in os.walk(output_dir)])\n",
    "print(f\"‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏õ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {num_files} ‡πÑ‡∏ü‡∏•‡πå!\")\n",
    "# Cleanup\n",
    "if os.path.exists(output_dir): shutil.rmtree(output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56d7432",
   "metadata": {},
   "source": [
    "### Lab Report Template (‡∏™‡πà‡∏á‡∏ó‡πâ‡∏≤‡∏¢‡∏Ñ‡∏≤‡∏ö)\n",
    "‡πÉ‡∏´‡πâ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô 1 ‡∏´‡∏ô‡πâ‡∏≤‡∏™‡∏±‡πâ‡∏ô ‡πÜ:\n",
    "- ‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå CSV vs Parquet (‡∏Å‡∏µ‡πà‡πÄ‡∏ó‡πà‡∏≤?)\n",
    "- ‡πÄ‡∏ß‡∏•‡∏≤‡∏≠‡πà‡∏≤‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß vs ‡∏ó‡∏±‡πâ‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á\n",
    "- ‡∏ß‡∏¥‡∏ò‡∏µ partition ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÅ‡∏•‡∏∞‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•\n",
    "- ‡∏Ç‡πâ‡∏≠‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á small files ‡∏´‡∏£‡∏∑‡∏≠ row group size\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbae6fc5",
   "metadata": {},
   "source": [
    "## Wrap-up & Post-test\n",
    "**Takeaways**\n",
    "1. File system ‡∏°‡∏µ block/fragmentation ‚Üí small files problem\n",
    "2. DB ‡∏ä‡∏ô‡∏∞‡πÑ‡∏ü‡∏•‡πå‡∏ï‡∏£‡∏á‡∏ó‡∏µ‡πà ACID + Index\n",
    "3. Stack = ‡∏£‡∏µ‡πÇ‡∏°‡∏ó, Heap = ‡∏ó‡∏µ‡∏ß‡∏µ, GC ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡∏≠‡∏á‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏£‡∏µ‡πÇ‡∏°‡∏ó‡∏ä‡∏µ‡πâ\n",
    "4. Parquet/Columnar = ‡πÄ‡∏•‡πá‡∏Å‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡πá‡∏ß‡∏î‡πâ‡∏ß‡∏¢ dictionary + RLE + column pruning + predicate pushdown\n",
    "5. Lakehouse = Parquet + Transaction Log (Delta/Iceberg/Hudi)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7199c29f",
   "metadata": {},
   "source": [
    "### After Class ‚Äî ‡∏≠‡πà‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡πÄ‡∏ß‡∏•‡∏≤\n",
    "- DuckDB Docs: Query Parquet & metadata, filter/column pushdown\n",
    "- AWS EMR/Athena Best Practices: ‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå 128‚Äì512MB, ‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á partition ‡∏•‡∏∂‡∏Å\n",
    "- Blog: Parquet predicate pushdown (row-group pruning, dictionary, bloom filter)\n",
    "- HDFS Small Files Problem: ‡∏ó‡∏≥‡πÑ‡∏° metadata ‡∏à‡∏∂‡∏á‡∏•‡πà‡∏°‡∏á‡πà‡∏≤‡∏¢ ‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ‡∏î‡πâ‡∏ß‡∏¢ compaction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af18a75",
   "metadata": {},
   "source": [
    "---\n",
    "# üìù Lab Report Template (‡∏™‡πà‡∏á‡∏ó‡πâ‡∏≤‡∏¢‡∏Ñ‡∏≤‡∏ö)\n",
    "**‡∏ä‡∏∑‡πà‡∏≠-‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•:** ....................................................... **‡∏£‡∏´‡∏±‡∏™‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤:** ..............................................\n",
    "\n",
    "### Part 1: Experimental Results\n",
    "‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏à‡∏≤‡∏Å Code ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ô‡πÉ‡∏ô Lab\n",
    "\n",
    "| ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á | ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå (‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç/‡∏´‡∏ô‡πà‡∏ß‡∏¢) |\n",
    "|---|---|\n",
    "| 1. ‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå CSV (MB) | _______________ |\n",
    "| 2. ‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå Parquet (MB) | _______________ |\n",
    "| 3. Parquet ‡πÄ‡∏•‡πá‡∏Å‡∏•‡∏á‡∏Å‡∏µ‡πà‡πÄ‡∏ó‡πà‡∏≤? (CSV / Parquet) | _______________ |\n",
    "| 4. ‡πÄ‡∏ß‡∏•‡∏≤‡∏≠‡πà‡∏≤‡∏ô CSV ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ) | _______________ |\n",
    "| 5. ‡πÄ‡∏ß‡∏•‡∏≤‡∏≠‡πà‡∏≤‡∏ô Parquet ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ) | _______________ |\n",
    "| 6. ‡πÄ‡∏ß‡∏•‡∏≤‡∏≠‡πà‡∏≤‡∏ô Parquet ‡πÅ‡∏Ñ‡πà 1 ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå (‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ) | _______________ |\n",
    "\n",
    "### Part 2: Critical Thinking\n",
    "**1. Partitioning Strategy:**\n",
    "‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Log ‡∏£‡∏≤‡∏¢‡∏ß‡∏±‡∏ô ‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Partition ‡∏î‡πâ‡∏ß‡∏¢‡∏≠‡∏∞‡πÑ‡∏£? ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á `date` vs `user_id`? ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏´‡∏ï‡∏∏‡πÉ‡∏î?\n",
    "> ‡∏ï‡∏≠‡∏ö: ....................................................................................................................................................................\n",
    "\n",
    "**2. Small Files Problem:**\n",
    "‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏≤ Save ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏∏‡∏Å‡πÜ 1 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ‡∏•‡∏á Data Lake ‡∏à‡∏∞‡πÄ‡∏Å‡∏¥‡∏î‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏≠‡∏∞‡πÑ‡∏£‡∏Ç‡∏∂‡πâ‡∏ô? ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏£‡πÅ‡∏Å‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?\n",
    "> ‡∏ï‡∏≠‡∏ö: ....................................................................................................................................................................\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cb8a6c",
   "metadata": {},
   "source": [
    "### Part 3: Code Challenge (‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡∏≥‡πÉ‡∏ô‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á)\n",
    "‡∏à‡∏á‡πÄ‡∏ï‡∏¥‡∏° Code ‡πÉ‡∏´‡πâ‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÑ‡∏ü‡∏•‡πå Parquet ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20428f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå Parquet ‡πÇ‡∏î‡∏¢‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'status' ‡πÅ‡∏•‡∏∞ 'value' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î RAM\n",
    "df = pd.read_parquet('data.parquet', columns=['_______', '_______'])\n",
    "\n",
    "# 2. ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÑ‡∏ü‡∏•‡πå Parquet ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏ö‡∏µ‡∏ö‡∏≠‡∏±‡∏î‡πÅ‡∏ö‡∏ö 'zstd' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
    "df.to_parquet('output.parquet', compression='_______')\n",
    "\n",
    "# 3. ‡πÉ‡∏ä‡πâ DuckDB ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÇ‡∏î‡∏¢‡πÅ‡∏ö‡πà‡∏á Partition ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'date'\n",
    "duckdb.query(\"COPY (SELECT * FROM my_table) TO 'my_data' (FORMAT 'parquet', PARTITION_BY ['_______'])\")\n",
    "\n",
    "# 4. ‡πÄ‡∏ä‡πá‡∏Ñ‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå (byte) ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ä‡∏∑‡πà‡∏≠ data.parquet\n",
    "file_size = os.path._______('data.parquet')\n",
    "\n",
    "# 5. (Concept) ‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Row Group ‡∏ó‡∏µ‡πà‡∏°‡∏µ status='Completed' ‡∏£‡∏∞‡∏ö‡∏ö‡πÉ‡∏ä‡πâ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏≠‡∏∞‡πÑ‡∏£‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡πÇ‡∏î‡∏î‡∏Ç‡πâ‡∏≤‡∏°?\n",
    "# ‡∏ï‡∏≠‡∏ö: ___________________ (Predicate Pushdown / Indexing / Caching)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  },
  "title": "Big Data Week 2"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
